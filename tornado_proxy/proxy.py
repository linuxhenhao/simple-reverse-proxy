#!/usr/bin/env python
# -*- coding: utf-8 -*-
#####################################################
# A tornado based reverse proxy with builtin filters and
# self dns resolve abilities
# ##################################################

from tornado import web, gen, httpclient
import sys
if sys.version_info[0] < 3:  # in python 2
    from urlparse import urlparse
    import Cookie
else:
    from urllib.parse import urlparse
    from http import cookies as Cookie
from . import utils


class ProxyHandler(web.RequestHandler):
    SUPPORTED_METHODS = ('GET', 'HEAD', 'POST')
    # for proxy purpose, get head and post methods are enough

    def initialize(self, configd, router, page_filters):
        # initiate a async http client for forwarding incoming requests
        self._client = httpclient.AsyncHTTPClient()
        self._configd = configd  # handler config dict
        self._router = router
        self._filters = page_filters

    def compute_etag(self):
        # disable tornado's etag
        return None

    @gen.coroutine
    def get(self):
        redirected = self._route.redirect(self.request)
        # the requested url not in our router, return 404 page
        if(redirected is None):
            self.set_status(404)
            self.write('<p align="center">page not found!</p>')
            self.finish()
        else:
            url, m_headers = redirected
            response = yield self._client.fetch(
                            url,
                            method=self.request.method,
                            follow_redirects=False,
                            headers = m_headers,
                            body=self.request.body
                            allow_ipv6=self._configd['allow_ipv6'])
            # set status code for tornado's response
            self.set_status(response.code)

            # modify response's headers and body using filter
            self._filters.filt(response)

            # modify returned headers,
            self._router.reverse_headers(response)
            r_headers = response.headers.get_all()
            for header in r_headers:
                self.set_header(header, r_headers[header])
            # content length will be automaticly generated by tornado
            self.write(response.body)
            self.finish()

    # using self.get to do post and head proxies
    post = get
    head = get


class Router(object):
    '''
    Router is used to redirect url and related headers of incoming requests,
    The redirection obay the rules defined in settings.py
    INPUT: request
    OUTPUT: target url and modifiled headers
    '''
    def __init__(self, rules):
        '''
        @param list rules: (origin_url, target_url) tuple in a list
        '''
        # gen rules dict to make host match more effective
        self._forward_rules, self._reverse_rules = self._gen_rules_d(rules)
        # using cookie_domain_d to store domain maps in cookie
        self._cookie_domain_d = dict()

    def _gen_rules_d(self, rules):
        '''
        @param list rules: rules [('scheme:host', 'scheme:target')]
        due to the fact that linear search one by one is so ineffective,
        convert rules into two different dict
        '''
        forward_dict = dict()
        reverse_dict = dict()
        for host_str, target_str in rules:
            forward_dict[host_str] = target_str
            reverse_dict[target_str] = host_str
        return forward_dict, reverse_dict

    def redirect(self, request):
        # the request object constructed from recieved request always do
        # not have scheme and host info
        host = request.headers['Host']
        proxy_host_str = ":".joint(request.protocol, host)

        # get target scheme:host string
        target_str = self._forward_rules.get(proxy_host_str, None)
        if(target_str is None):
            return None
        else:
            # using maxsplit=1 to avoid unwanted split in url if other colon
            # exists in url
            parse_result = urlparse(request.uri)
            if(parse_result.netloc == ''):
                # request.uri do not contain https?://host part,
                # only path portion has been saved in request.uri
                scheme, target_host = target_str.split(":", maxsplit=1)
                url = scheme + "://" + target_host + request.uri
            else:
                # request.uri is a full functional url
                url = request.uri
            return (url, headers)

    def reverse(self, response):
        # real url to proxy server url
        headers = dict()
        for header in response.headers.get_all():
            if header not in ('Content-Length', 'Transfer-Encoding',
                              'Content-Encoding', 'Connection'):
                # let tornado set these parameters, do not use those in
                # response.headers
                headers[header]  = response.headers[header]
        target_url = response.effective_url
        parse_result = urlparse(target_url)
        target_host_str = ":".join(parse_result.scheme, parse_result.netloc)

        # get proxy host url
        proxy_host_str = self._reverse_rules[target_host_str]
        path_parameters_portion = utils.path_parameters_from_url(response.request.url)
        # the response.request is an instance of httpclient.HTTPRequest
        # not HTTPServerRequest, so url is used here
        scheme, left_part = proxy_host_str.split(':', maxsplit=1)
        # because proxy host str may in 'https:proxyhost.com/xxx/' format,
        # name the other part of the splited results to left_part, not proxy_host
        host_url = scheme + "://" + left_part + path_parameters_portion
        return (host_url, headers)

    def revers_headers(self, response):
        '''
        @param tornado.httpclient.HTTPResponse response; cookie domain and
            location in response.headers should be modified to fit proxy.
        '''
        parse_result = urlparse(response.effective_url)
        real_server_str = ":".join(parse_result.scheme, parse_result.netloc)
        proxy_host_str = self._reverse_rules[real_server_str]
        # proxy_host_str eg: 'https:hahah.com/test/'
        # if there is not /test/ part, split('/', maxsplit=1) returns ['hahah.com']
        # else: split('/', maxsplit=1) returns ['hahah.com', 'test/']
        # so, using index [0] to get the host part
        proxy_domain = proxy_host_str.split(":")[1].split('/', maxsplit=1)[0]
        self._cookie_domain_replace(response.headers, proxy_domain)
        # Because python using references, the change in self._cookie_domain_r-
        # eplace will be saved

        # if the remote real server returned a location header, replace
        if('Location' in response.headers):
            new_url = response.headers['Location']
            parse_result = urlparse(new_url)
            target_host_str = ":".join(parse_result.scheme, parse_result.netloc)
            if(target_host_str in self._reverse_rules):
                # reverse_rules: real server to proxy server mapping
                proxy_host_str = self._reverse_rules[target_host_str]
                scheme, partial_url = proxy_host_str.split(":", maxsplit=1)
                proxy_url = scheme + "://" + partial_url +\
                                utils.path_parameters_from_url(proxy_host_str)
                response.headers['Location'] = proxy_url


    def _cookie_domain_replace(self, headers, target_domain):
        '''
        @param dict_like headers: headers may contain "Cookie" or "Set-Cookie"
        @param str target_domain: replace domain section using target_domain

        The browser only accept set-cookie header when the domain value in the
        set-cookie section (if exists) matched with current viewing server domain.

        due to the above facts, we should do cookie domain replace for both
        request and response:
        the domain replace actions are very clear, when a set-cookie from remote
        real server comes in, replace the domain to proxy server's domain. when
        a request from a browser comes in, replace the cookie's domain to remote
        real server's domain
        '''
        # both HTTPServerRequest.headers and client.HTTPResponse.headers
        # are the instance of tornado.httputil.HTTPResponse, all the keys
        # in headers are normalized, says in capital form
        cookie_instance = Cookie.SimpleCookie()
        cookie_section_name = False
        if('Set-Cookie' in headers):
            # its a response header, to make the browser accept
            # this set-cookie action, we should replace the domain to proxy host
            cooke_section_name = 'Set-Cookie'
        elif('Cookie' in headers):
            cooke_section_name = 'Cookie'

        if(cookie_section_name):
            cookie_instance.load(headers['Set-Cookie'])
            for key in cookie_instance:
                domain = cookie_instance[key].pop('domain', False)
                if(domain):
                    # has domain part in cookie
                    cookie_instance[key]['domain'] = target_domain
            headers[cookie_section_name] =\
                cookie_instance.output(header='').strip()
